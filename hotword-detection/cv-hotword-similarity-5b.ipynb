{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ff92bb3-ba96-478b-8ce2-a4c9693f2a92",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6029d9-2579-4b50-b8ed-86ad9ef1e4c3",
   "metadata": {},
   "source": [
    "### Install required libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cc2407-04ee-416e-ac25-04d9af663eb9",
   "metadata": {},
   "source": [
    "The libraries should already be installed in the terminal after running \n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "in the root directory, but this will be useful if the notebook is used in other environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "099baa83-fd38-4f7b-b784-fdef97746c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install InstructorEmbedding==1.0.1\n",
    "!pip install scikit-learn==1.6.1\n",
    "!pip install pandas==2.2.3\n",
    "!pip install numpy==2.2.6\n",
    "!pip install sentence-transformers==2.2.2\n",
    "!pip install requests==2.32.3\n",
    "!pip install transformers==4.37.2\n",
    "!pip install huggingface-hub==0.25.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2769f81a-7918-40d8-b920-dc333d72c9c9",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ede0d6ed-e489-46fe-b9c1-b1cc5e9cd4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gordon.oh/Desktop/htx-xdata-asr/venv/lib/python3.13/site-packages/InstructorEmbedding/instructor.py:7: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import trange\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import requests\n",
    "from huggingface_hub import configure_http_backend\n",
    "import urllib3\n",
    "\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bbf9ca-5cc3-4c87-b4bb-d9b58198f773",
   "metadata": {},
   "source": [
    "### Other configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fefee4db-0450-4814-823d-11f5e7a17301",
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# [OPTIONAL] Use if there is SSL certificate verification issues\n",
    "def backend_factory() -> requests.Session:\n",
    "    session = requests.Session()\n",
    "    session.verify = False\n",
    "    return session\n",
    "\n",
    "\n",
    "configure_http_backend(backend_factory=backend_factory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d18393e-a2d2-4ada-8e31-245816f51d8b",
   "metadata": {},
   "source": [
    "### Load the Instructor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "585cdcc9-71df-40a2-88dd-b78a4eef07b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "model = INSTRUCTOR('hkunlp/instructor-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8a241d3-ca3e-4aed-ae9f-c6b6541ac458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>up_votes</th>\n",
       "      <th>down_votes</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>accent</th>\n",
       "      <th>duration</th>\n",
       "      <th>generated_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cv-valid-dev/sample-000000.mp3</td>\n",
       "      <td>be careful with your prognostications said the stranger</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BE CAREFUL WITH YOUR PROGNOSTICATIONS SAID THE STRANGER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cv-valid-dev/sample-000001.mp3</td>\n",
       "      <td>then why should they be surprised when they see one</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>THEN WHY SHOULD THEY BE SURPRISED WHEN THEY SEE ONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cv-valid-dev/sample-000002.mp3</td>\n",
       "      <td>a young arab also loaded down with baggage entered and greeted the englishman</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A YOUNG ARAB ALSO LOADED DOWN WITH BAGGAGE ENTERED AND GREETED THE ENGLISHMAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cv-valid-dev/sample-000003.mp3</td>\n",
       "      <td>i thought that everything i owned would be destroyed</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I FELT THAT EVERYTHING I OWNED WOULD BE DESTROYED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cv-valid-dev/sample-000004.mp3</td>\n",
       "      <td>he moved about invisible but everyone could hear him</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>fourties</td>\n",
       "      <td>female</td>\n",
       "      <td>england</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HE MOVED ABOUT INVISIBLE BUT EVERY ONE COULD HEAR HIM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         filename  \\\n",
       "0  cv-valid-dev/sample-000000.mp3   \n",
       "1  cv-valid-dev/sample-000001.mp3   \n",
       "2  cv-valid-dev/sample-000002.mp3   \n",
       "3  cv-valid-dev/sample-000003.mp3   \n",
       "4  cv-valid-dev/sample-000004.mp3   \n",
       "\n",
       "                                                                            text  \\\n",
       "0                        be careful with your prognostications said the stranger   \n",
       "1                            then why should they be surprised when they see one   \n",
       "2  a young arab also loaded down with baggage entered and greeted the englishman   \n",
       "3                           i thought that everything i owned would be destroyed   \n",
       "4                           he moved about invisible but everyone could hear him   \n",
       "\n",
       "   up_votes  down_votes       age  gender   accent  duration  \\\n",
       "0         1           0       NaN     NaN      NaN       NaN   \n",
       "1         2           0       NaN     NaN      NaN       NaN   \n",
       "2         2           0       NaN     NaN      NaN       NaN   \n",
       "3         3           0       NaN     NaN      NaN       NaN   \n",
       "4         1           0  fourties  female  england       NaN   \n",
       "\n",
       "                                                                  generated_text  \n",
       "0                        BE CAREFUL WITH YOUR PROGNOSTICATIONS SAID THE STRANGER  \n",
       "1                            THEN WHY SHOULD THEY BE SURPRISED WHEN THEY SEE ONE  \n",
       "2  A YOUNG ARAB ALSO LOADED DOWN WITH BAGGAGE ENTERED AND GREETED THE ENGLISHMAN  \n",
       "3                              I FELT THAT EVERYTHING I OWNED WOULD BE DESTROYED  \n",
       "4                          HE MOVED ABOUT INVISIBLE BUT EVERY ONE COULD HEAR HIM  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_csv_file = f'../asr/cv-valid-dev-with-generated-text.csv'\n",
    "df = pd.read_csv(cv_csv_file)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93f2c627-e93a-46e2-bbef-2d4c5b50475c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_words = ['BE CAREFUL', 'DESTROY', 'STRANGER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fc2d5af-8196-4ba5-9997-bb865bcbf4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Represent the warning concept:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8731ac27-9eb9-4f18-9e14-6b253690d275",
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_word_embeddings = model.encode([[instruction, hw] for hw in hot_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181326b3-1640-4deb-9120-8c10517815fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5978842-d805-49e8-a69c-f0e2f141d920",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'strip'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m sentences = df[\u001b[33m'\u001b[39m\u001b[33mgenerated_text\u001b[39m\u001b[33m'\u001b[39m].tolist()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m sentence_embeddings = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43minstruction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msentences\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/htx-xdata-asr/venv/lib/python3.13/site-packages/InstructorEmbedding/instructor.py:535\u001b[39m, in \u001b[36mINSTRUCTOR.encode\u001b[39m\u001b[34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m start_index \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(sentences), batch_size, desc=\u001b[33m\"\u001b[39m\u001b[33mBatches\u001b[39m\u001b[33m\"\u001b[39m, disable=\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar):\n\u001b[32m    534\u001b[39m     sentences_batch = sentences_sorted[start_index:start_index+batch_size]\n\u001b[32m--> \u001b[39m\u001b[32m535\u001b[39m     features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    536\u001b[39m     features = batch_to_device(features, device)\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/htx-xdata-asr/venv/lib/python3.13/site-packages/sentence_transformers/SentenceTransformer.py:319\u001b[39m, in \u001b[36mSentenceTransformer.tokenize\u001b[39m\u001b[34m(self, texts)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: Union[List[\u001b[38;5;28mstr\u001b[39m], List[Dict], List[Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]]):\n\u001b[32m    316\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    317\u001b[39m \u001b[33;03m    Tokenizes the texts\u001b[39;00m\n\u001b[32m    318\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_first_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/htx-xdata-asr/venv/lib/python3.13/site-packages/InstructorEmbedding/instructor.py:340\u001b[39m, in \u001b[36mINSTRUCTOR_Transformer.tokenize\u001b[39m\u001b[34m(self, texts)\u001b[39m\n\u001b[32m    338\u001b[39m         new_texts.append([s[\u001b[32m0\u001b[39m],s[\u001b[32m1\u001b[39m].strip().lower()])\n\u001b[32m    339\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m         new_texts.append([s[\u001b[32m0\u001b[39m], \u001b[43ms\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstrip\u001b[49m()])\n\u001b[32m    341\u001b[39m texts = new_texts\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(texts[\u001b[32m0\u001b[39m])==\u001b[32m2\u001b[39m,\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mThe input should have both instruction and input text\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[31mAttributeError\u001b[39m: 'float' object has no attribute 'strip'"
     ]
    }
   ],
   "source": [
    "# sentences = df['generated_text'].tolist()\n",
    "# sentence_embeddings = model.encode([[instruction, s] for s in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5dabf1-b96e-40d1-ae03-66094c6cf4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.45  # adjust based on desired sensitivity\n",
    "similarity_flags = []\n",
    "\n",
    "for emb in sentence_embeddings:\n",
    "    sims = cosine_similarity([emb], hotword_embeddings)[0]\n",
    "    is_similar = any(sim > threshold for sim in sims)\n",
    "    similarity_flags.append(is_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde9dfda-aa85-47d1-b3ee-3e64a3f3b638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarities\n",
    "similarities = cosine_similarity(vectors)\n",
    "\n",
    "# Turn into a dataframe\n",
    "pd.DataFrame(similarities,\n",
    "            index=sentences,\n",
    "            columns=sentences) \\\n",
    "            .style \\\n",
    "            .background_gradient(axis=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
